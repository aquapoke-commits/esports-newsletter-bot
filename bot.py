import discord
from discord.ext import commands
import requests
from bs4 import BeautifulSoup
import feedparser
import html
import os
import re 
from datetime import datetime, timedelta, timezone
import asyncio

# =====================================================================
# [ë³´ì•ˆ ì„¤ì •]
# =====================================================================
if 'DISCORD_TOKEN' in os.environ:
    DISCORD_TOKEN = os.environ['DISCORD_TOKEN']
else:
    print("âš ï¸ ì—ëŸ¬: DISCORD_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# [ì„¤ì •] ì±„ë„ ID ë¦¬ìŠ¤íŠ¸
TARGET_CHANNELS = [
    1447898781365567580, # GGX Proto
    1450833963278012558, # Hanta.GG
    # 987654321098765432, 
]

# =====================================================================
# [â˜…ì¤‘ìš”â˜…] í‚¤ì›Œë“œ ë ˆë²¨ ì„¤ì •
# =====================================================================

# ğŸ‘‘ 1. í”„ë¦¬ë¯¸ì—„ í‚¤ì›Œë“œ (ì œëª©ì— 1ê°œë§Œ ìˆì–´ë„ ë¬´ì¡°ê±´ ì„ ë³„)
# -> í•µì‹¬ ì„ ìˆ˜, ì¸ê¸° íŒ€, ë§¤ìš° ì¤‘ìš”í•œ ëŒ€íšŒ ëª…ì¹­ ë“±
PREMIUM_KEYWORDS = [
    "Faker", "í˜ì´ì»¤", "T1", "í‹°ì›", 
    "World Championship", "ë¡¤ë“œì»µ", "MSI", 
    "Zeus", "Oner", "Gumayusi", "Keria", # ì œì˜¤êµ¬ì¼€
    "Chovy", "ShowMaker", "Ruler", "Viper" # ìŠˆí¼ìŠ¤íƒ€
]

# ğŸ§¢ 2. ì¼ë°˜ í‚¤ì›Œë“œ (ì œëª©ì— 2ê°œ ì´ìƒ ìˆì–´ì•¼ ì„ ë³„)
# -> ë¦¬ê·¸ ì´ë¦„, ì¼ë°˜ íŒ€ëª…, í”í•œ ì´ìŠ¤í¬ì¸  ìš©ì–´
NORMAL_KEYWORDS = [
    "ì´ìŠ¤í¬ì¸ ", "e-sports", "LoL", "League of Legends",
    "LCK", "LPL", "LEC", "LCS", "VCT", "ë°œë¡œë€íŠ¸", "PUBG", "ë°°í‹€ê·¸ë¼ìš´ë“œ", "ì´í„°ë„ ë¦¬í„´",
    "Gen.G", "ì  ì§€", "HLE", "í•œí™”ìƒëª…", "DK", "ë””í”ŒëŸ¬ìŠ¤", "KT", "DRX", "FOX", "NS", "BRO",
    "ìš°ìŠ¹", "ê²°ìŠ¹", "í”Œë ˆì´ì˜¤í”„", "ê°œë§‰", "ì¸í„°ë·°", "ë‹¨ë…", "ì†ë³´", "ì˜¤í”¼ì…œ"
]

# (ê²€ìƒ‰ìš©) ë´‡ì€ ì´ ë‘ ë¦¬ìŠ¤íŠ¸ë¥¼ í•©ì³ì„œ ê²€ìƒ‰ì— ì‚¬ìš©í•©ë‹ˆë‹¤.
SEARCH_KEYWORDS = list(set(PREMIUM_KEYWORDS + NORMAL_KEYWORDS))

# =====================================================================

# [ì„¤ì •] ì°¨ë‹¨í•  ë‹¨ì–´
EXCLUDE_LIST = ["theqoo", "ë”ì¿ ", "instiz", "ì¸ìŠ¤í‹°ì¦ˆ", "fmkorea", "í¨ì½”", "dcinside", "ë””ì‹œ", "ë°”ì¹´ë¼", "í† í† ", "ì¹´ì§€ë…¸", "ìŠ¬ë¡¯", "MSN", "ì¸ë²¤", "ë³´í†µì£¼", "íŒ¨ì¹˜ë…¸íŠ¸", "ì‚¬ëª¨ëŒ€ì¶œ", "investing","vietnam", "ZUM"]

# [ì„¤ì •] ë‰´ìŠ¤ ìœ íš¨ ì‹œê°„
MAX_HOURS = 24

# ë´‡ ì„¤ì •
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix="!", intents=intents)

# ---------------------------------------------------
# [í•¨ìˆ˜ 0] ê³¼ê±° ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
# ---------------------------------------------------
async def get_past_titles(channel_id):
    print("â³ ì–´ì œ ë³´ë‚¸ ë‰´ìŠ¤ ê¸°ë¡ì„ í™•ì¸í•˜ëŠ” ì¤‘...")
    past_titles = []
    channel = bot.get_channel(channel_id)
    
    if not channel:
        print("âš ï¸ ê¸°ë¡ì„ í™•ì¸í•  ì±„ë„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return []

    try:
        async for message in channel.history(limit=10):
            if message.author == bot.user:
                for embed in message.embeds:
                    if embed.description:
                        matches = re.findall(r"\[(.*?)\]\(http", embed.description)
                        past_titles.extend(matches)
        print(f"ğŸ§  ê¸°ì–µ ì™„ë£Œ: ê³¼ê±° ë‰´ìŠ¤ ì œëª© {len(past_titles)}ê°œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return past_titles
    except Exception as e:
        print(f"âš ï¸ ê³¼ê±° ê¸°ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []

# ---------------------------------------------------
# [í•¨ìˆ˜ 0.5] í‚¤ì›Œë“œ ë ˆë²¨ íŒë…ê¸° (í•µì‹¬ ì•Œê³ ë¦¬ì¦˜)
# ---------------------------------------------------
def check_keyword_level(title):
    # 1. í”„ë¦¬ë¯¸ì—„ í‚¤ì›Œë“œ ê²€ì‚¬ (1ê°œë§Œ ìˆì–´ë„ í•©ê²©)
    for p_key in PREMIUM_KEYWORDS:
        # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ì‚¬í•˜ë ¤ë©´ lower() ì‚¬ìš©
        if p_key.lower() in title.lower():
            return True, f"ğŸ‘‘í”„ë¦¬ë¯¸ì—„({p_key})"

    # 2. ì¼ë°˜ í‚¤ì›Œë“œ ê²€ì‚¬ (2ê°œ ì´ìƒ ìˆì–´ì•¼ í•©ê²©)
    count = 0
    matched = []
    for n_key in NORMAL_KEYWORDS:
        if n_key.lower() in title.lower():
            count += 1
            matched.append(n_key)
            
    if count >= 2:
        return True, f"ğŸ§¢ì¼ë°˜í•©ê²©({', '.join(matched)})"

    return False, f"ì¡°ê±´ë¯¸ë‹¬(ì¼ë°˜ {count}ê°œ)"

# ---------------------------------------------------
# [í¬ë¡¤ë§ í•¨ìˆ˜ 1] ë„¤ì´ë²„ ë‰´ìŠ¤
# ---------------------------------------------------
def get_naver_news(keyword):
    news_list = []
    clean_keyword = keyword.replace(" ", "+")
    url = f"https://search.naver.com/search.naver?where=news&query={clean_keyword}&sort=1"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')
        items = soup.select('ul.list_news > li.bx')
        
        for item in items:
            title_tag = item.select_one('a.news_tit')
            if not title_tag: continue
            
            title = title_tag.text
            link = title_tag['href']
            
            info_group = item.select('.info_group .info')
            is_recent = False
            time_log = "ì•Œìˆ˜ì—†ìŒ"
            
            for info in info_group:
                text = info.text
                if "ë¶„ ì „" in text or "ì‹œê°„ ì „" in text:
                    time_log = text 
                    if "ì¼ ì „" in text:
                        is_recent = False
                        break
                    is_recent = True
                    break
            
            if is_recent:
                news_list.append({
                    "title": title, 
                    "link": link, 
                    "source": "Naver", 
                    "origin": "ë„¤ì´ë²„",
                    "time_str": time_log,
                    "search_keyword": keyword
                })

    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ ì˜¤ë¥˜({keyword}): {e}")
        pass
    return news_list

# ---------------------------------------------------
# [í¬ë¡¤ë§ í•¨ìˆ˜ 2] êµ¬ê¸€ ë‰´ìŠ¤
# ---------------------------------------------------
def get_google_news(keyword):
    news_list = []
    clean_keyword = keyword.replace(" ", "+")
    url = f"https://news.google.com/rss/search?q={clean_keyword}+when:1d&hl=ko&gl=KR&ceid=KR:ko"
    
    PAST_YEARS = ["2020", "2021", "2022", "2023", "2024", "2025"] 

    try:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            if not hasattr(entry, 'published_parsed') or entry.published_parsed is None:
                continue
            
            try:
                pub_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                current_date = datetime.now(timezone.utc)
                
                diff_seconds = (current_date - pub_date).total_seconds()
                diff_hours = diff_seconds / 3600
                if diff_hours < 0: diff_hours = 0
                
                pub_date_kst = pub_date + timedelta(hours=9)
                time_str_kst = pub_date_kst.strftime("%Y-%m-%d %H:%M:%S")

                source_name = "Google"
                if hasattr(entry, 'source') and hasattr(entry.source, 'title'):
                    source_name = entry.source.title

                if diff_hours > MAX_HOURS:
                    continue
                
                is_old_title = False
                for year in PAST_YEARS:
                    if year in entry.title:
                         is_old_title = True
                         print(f"ğŸ“… [êµ¬ê¸€|ì—°ë„íƒˆë½] {entry.title} (ì´ìœ : ê³¼ê±° ì—°ë„ '{year}' í¬í•¨)")
                         break
                if is_old_title: continue

                news_list.append({
                    "title": entry.title,
                    "link": entry.link,
                    "source": source_name,
                    "origin": "êµ¬ê¸€",
                    "time_str": time_str_kst,
                    "search_keyword": keyword
                })
                
            except:
                continue
                
    except Exception as e:
        print(f"âŒ êµ¬ê¸€ ì˜¤ë¥˜({keyword}): {e}")
        pass
        
    return news_list

# ---------------------------------------------------
# [í†µí•© í•¨ìˆ˜] ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì„ ë³„ (í‚¤ì›Œë“œ ë ˆë²¨ ì ìš©)
# ---------------------------------------------------
def collect_news(past_titles):
    print(f"\nğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì •ë°€ ì‹¬ì‚¬ ì‹œì‘ (ì œí•œ: {MAX_HOURS}ì‹œê°„)")
    all_news = []
    seen_links = set()
    collected_titles = [] 
    
    MAX_TOTAL = 20        
    MAX_PER_KEYWORD = 4
    DUPLICATE_THRESHOLD = 9 
    
    # ê²€ìƒ‰ì€ ëª¨ë“  í‚¤ì›Œë“œ(SEARCH_KEYWORDS)ë¡œ ìˆ˜í–‰
    for keyword in SEARCH_KEYWORDS:
        if len(all_news) >= MAX_TOTAL: 
            print("ğŸ›‘ [ì „ì²´ì œí•œ] ì´ 20ê°œë¥¼ ëª¨ë‘ ì±„ì›Œ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
            
        n_res = get_naver_news(keyword)
        g_res = get_google_news(keyword)
        
        current_keyword_count = 0
        
        for news in n_res + g_res:
            if len(all_news) >= MAX_TOTAL: break
            if current_keyword_count >= MAX_PER_KEYWORD: break
            
            # [1] ì°¨ë‹¨ ì‚¬ì´íŠ¸ í•„í„°
            is_excluded = False
            check_target = (news['link'] + news['title'] + news.get('source', '')).lower()
            
            for ban_word in EXCLUDE_LIST:
                if ban_word.lower() in check_target:
                    is_excluded = True
                    print(f"ğŸš« [ì‚¬ì´íŠ¸ì°¨ë‹¨][{news['origin']}] {news['title']} (ì´ìœ : {ban_word})") 
                    break
            
            if is_excluded: continue 
            
            # [1.5] â˜… í‚¤ì›Œë“œ ë ˆë²¨(Premium/Normal) í•„í„° â˜…
            # ì—¬ê¸°ì„œ ì œëª©ì„ ê²€ì‚¬í•´ì„œ í•©ê²© ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
            is_qualified, qualify_reason = check_keyword_level(news['title'])
            
            if not is_qualified:
                # ë¡œê·¸ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì´ ì¤„ì„ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”
                # print(f"ğŸ“‰ [ì¡°ê±´ë¯¸ë‹¬] {news['title']} (ì‚¬ìœ : {qualify_reason})")
                continue

            # [2] ë§í¬ ì¤‘ë³µ í•„í„°
            if news['link'] in seen_links: continue

            clean_title = html.unescape(news['title']).replace("[", "").replace("]", "").strip()
            
            # [3] ì œëª© ë‚´ìš© ì¤‘ë³µ í•„í„°
            is_similar = False
            match_cause = "" 
            for existing_title in collected_titles:
                if len(clean_title) < DUPLICATE_THRESHOLD: break
                for i in range(len(clean_title) - DUPLICATE_THRESHOLD + 1):
                    sub_string = clean_title[i : i + DUPLICATE_THRESHOLD]
                    if sub_string in existing_title:
                        is_similar = True
                        match_cause = sub_string 
                        break 
                if is_similar: break
            
            if is_similar:
                print(f"ğŸ”— [ë‚´ìš©ì¤‘ë³µ][{news['origin']}] {clean_title} (ê²¹ì¹œë‹¨ì–´: '{match_cause}')")
                continue
            
            # [4] ê³¼ê±° ê¸°ë¡ ì¤‘ë³µ í•„í„°
            is_past_duplicate = False
            past_match_cause = "" 
            matched_past_title = "" 
            
            for past_title in past_titles:
                if len(clean_title) < DUPLICATE_THRESHOLD or len(past_title) < DUPLICATE_THRESHOLD:
                    break
                
                for i in range(len(clean_title) - DUPLICATE_THRESHOLD + 1):
                    sub_string = clean_title[i : i + DUPLICATE_THRESHOLD]
                    if sub_string in past_title:
                        is_past_duplicate = True
                        past_match_cause = sub_string 
                        matched_past_title = past_title 
                        break
                if is_past_duplicate: break
                
            if is_past_duplicate:
                print(f"ğŸ§Ÿ [ì–´ì œë‰´ìŠ¤ì¤‘ë³µ] {clean_title} (ê²¹ì¹œë‹¨ì–´: '{past_match_cause}')")
                continue

            # [5] ìµœì¢… í•©ê²© (í•©ê²© ì‚¬ìœ  í•¨ê»˜ ì¶œë ¥)
            print(f"âœ… [{qualify_reason}][{news['origin']}] {clean_title}")
            
            all_news.append({"title": clean_title, "link": news['link']})
            seen_links.add(news['link'])
            collected_titles.append(clean_title)
            current_keyword_count += 1
                
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(all_news)}ê°œ ë‰´ìŠ¤ ì „ì†¡ ì¤€ë¹„ ì™„ë£Œ\n")
    return all_news

# ---------------------------------------------------
# [ì „ì†¡ ë¡œì§]
# ---------------------------------------------------
async def send_newsletter(target_channel_id, news_data):
    channel = bot.get_channel(target_channel_id)
    if not channel:
        print(f"âŒ ì±„ë„ ì—†ìŒ: {target_channel_id}")
        return

    if not news_data:
        return

    today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    MAX_DESCRIPTION_LEN = 3500
    current_description = ""
    page_count = 1
    
    embed = discord.Embed(title=f"ğŸ® {today} ì´ìŠ¤í¬ì¸  ì£¼ìš” ì†Œì‹", color=0x00ff00)

    for idx, news in enumerate(news_data):
        one_line = f"` {idx+1}. ` [{news['title']}]({news['link']})\n\n"
        
        if len(current_description) + len(one_line) > MAX_DESCRIPTION_LEN:
            embed.description = current_description
            embed.set_footer(text=f"HantaGG NewsBot â€¢ {page_count}í˜ì´ì§€")
            await channel.send(embed=embed)
            page_count += 1
            current_description = ""
            embed = discord.Embed(color=0x00ff00)
            
        current_description += one_line

    if current_description:
        embed.description = current_description
        embed.set_footer(text=f"HantaGG NewsBot â€¢ ë§ˆì§€ë§‰ í˜ì´ì§€ (ì´ {len(news_data)}ê±´)")
        await channel.send(embed=embed)

    print(f"âœ… ì „ì†¡ ì™„ë£Œ: {target_channel_id}")

# ---------------------------------------------------
# [ë´‡ ì‹¤í–‰]
# ---------------------------------------------------
@bot.event
async def on_ready():
    print(f"âœ… ë´‡ ë¡œê·¸ì¸: {bot.user}")
    
    try:
        past_titles = []
        if TARGET_CHANNELS:
            past_titles = await get_past_titles(TARGET_CHANNELS[0])
            
        todays_news = collect_news(past_titles)
        
        for channel_id in TARGET_CHANNELS:
            await send_newsletter(channel_id, todays_news)
            
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    print("ğŸ‘‹ ì„ë¬´ ì™„ë£Œ. ì¢…ë£Œí•©ë‹ˆë‹¤.")
    await bot.close()

if __name__ == "__main__":
    bot.run(DISCORD_TOKEN)
