import discord
from discord.ext import commands
import requests
from bs4 import BeautifulSoup
import feedparser
import html
import os
import re # [ì¶”ê°€] ì •ê·œí‘œí˜„ì‹ (ë©”ì‹œì§€ì—ì„œ ì œëª©ë§Œ ë½‘ì•„ë‚´ê¸° ìœ„í•´ í•„ìš”)
from datetime import datetime, timedelta, timezone
import asyncio

# =====================================================================
# [ë³´ì•ˆ ì„¤ì •]
# =====================================================================
if 'DISCORD_TOKEN' in os.environ:
    DISCORD_TOKEN = os.environ['DISCORD_TOKEN']
else:
    print("âš ï¸ ì—ëŸ¬: DISCORD_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# [ì„¤ì •] ì±„ë„ ID ë¦¬ìŠ¤íŠ¸
TARGET_CHANNELS = [
    1447898781365567580, # GGX Proto
    1450833963278012558, # Hanta.GG
    # 987654321098765432,  # í…ŒìŠ¤íŠ¸ìš©
]

# [ì„¤ì •] ê²€ìƒ‰ì–´ ëª©ë¡
KEYWORDS = ["ì´ìŠ¤í¬ì¸ ", "LCK", "VCT", "ì´í„°ë„ ë¦¬í„´ ì´ìŠ¤í¬ì¸ ", "PUBG", "í‹°ì›", "Faker", "Gen.G", "HLE", "kt Rolster", "ë””í”ŒëŸ¬ìŠ¤ ê¸°ì•„", "í”¼ì–´ì—‘ìŠ¤", "ë†ì‹¬ ë ˆë“œí¬ìŠ¤", "í•œì§„ ë¸Œë¦¬ì˜¨", "DRX", "DN SOOPers"]

# [ì„¤ì •] ì°¨ë‹¨í•  ë‹¨ì–´
EXCLUDE_LIST = ["theqoo", "ë”ì¿ ", "instiz", "ì¸ìŠ¤í‹°ì¦ˆ", "fmkorea", "í¨ì½”", "dcinside", "ë””ì‹œ", "ë°”ì¹´ë¼", "í† í† ", "ì¹´ì§€ë…¸", "ìŠ¬ë¡¯", "MSN", "ì¸ë²¤", "ë³´í†µì£¼", "íŒ¨ì¹˜ë…¸íŠ¸", "ì‚¬ëª¨ëŒ€ì¶œ", "investing","vietnam", "ZUM"]

# [ì„¤ì •] ë‰´ìŠ¤ ìœ íš¨ ì‹œê°„
MAX_HOURS = 24

# ë´‡ ì„¤ì •
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix="!", intents=intents)

# ---------------------------------------------------
# [í•¨ìˆ˜ 0] ê³¼ê±° ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸° (ê¸°ì–µë ¥ ì¶”ê°€)
# ---------------------------------------------------
async def get_past_titles(channel_id):
    print("â³ ì–´ì œ ë³´ë‚¸ ë‰´ìŠ¤ ê¸°ë¡ì„ í™•ì¸í•˜ëŠ” ì¤‘...")
    past_titles = []
    channel = bot.get_channel(channel_id)
    
    if not channel:
        print("âš ï¸ ê¸°ë¡ì„ í™•ì¸í•  ì±„ë„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return []

    try:
        # ìµœê·¼ ë©”ì‹œì§€ 5ê°œë§Œ ì½ì–´ì™€ë„ ì¶©ë¶„í•¨ (ì–´ì œ ë‰´ìŠ¤ë ˆí„°ê°€ ê·¸ ì•ˆì— ìˆì„ í…Œë‹ˆê¹Œ)
        async for message in channel.history(limit=5):
            # ë´‡ì´ ë³´ë‚¸ ë©”ì‹œì§€ë§Œ í™•ì¸
            if message.author == bot.user:
                for embed in message.embeds:
                    if embed.description:
                        # ì •ê·œì‹ìœ¼ë¡œ [ì œëª©](ë§í¬) í˜•íƒœì—ì„œ 'ì œëª©'ë§Œ ì¶”ì¶œ
                        # íŒ¨í„´: [ê¸€ì] -> ê¸€ìë§Œ ë½‘ì•„ëƒ„
                        matches = re.findall(r"\[(.*?)\]\(http", embed.description)
                        past_titles.extend(matches)
                        
        print(f"ğŸ§  ê¸°ì–µ ì™„ë£Œ: ê³¼ê±° ë‰´ìŠ¤ ì œëª© {len(past_titles)}ê°œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return past_titles
        
    except Exception as e:
        print(f"âš ï¸ ê³¼ê±° ê¸°ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []

# ---------------------------------------------------
# [í¬ë¡¤ë§ í•¨ìˆ˜ 1] ë„¤ì´ë²„ ë‰´ìŠ¤
# ---------------------------------------------------
def get_naver_news(keyword):
    news_list = []
    clean_keyword = keyword.replace(" ", "+")
    url = f"https://search.naver.com/search.naver?where=news&query={clean_keyword}&sort=1"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')
        items = soup.select('ul.list_news > li.bx')
        
        for item in items:
            title_tag = item.select_one('a.news_tit')
            if not title_tag: continue
            
            title = title_tag.text
            link = title_tag['href']
            
            info_group = item.select('.info_group .info')
            is_recent = False
            time_log = "ì•Œìˆ˜ì—†ìŒ"
            
            for info in info_group:
                text = info.text
                if "ë¶„ ì „" in text or "ì‹œê°„ ì „" in text:
                    time_log = text 
                    if "ì¼ ì „" in text:
                        is_recent = False
                        break
                    is_recent = True
                    break
            
            if is_recent:
                news_list.append({
                    "title": title, 
                    "link": link, 
                    "source": "Naver", 
                    "origin": "ë„¤ì´ë²„",
                    "time_str": time_log,
                    "keyword": keyword
                })

    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ ì˜¤ë¥˜({keyword}): {e}")
        pass
    return news_list

# ---------------------------------------------------
# [í¬ë¡¤ë§ í•¨ìˆ˜ 2] êµ¬ê¸€ ë‰´ìŠ¤
# ---------------------------------------------------
def get_google_news(keyword):
    news_list = []
    clean_keyword = keyword.replace(" ", "+")
    url = f"https://news.google.com/rss/search?q={clean_keyword}+when:1d&hl=ko&gl=KR&ceid=KR:ko"
    
    PAST_YEARS = ["2020", "2021", "2022", "2023", "2024", "2025"] 

    try:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            if not hasattr(entry, 'published_parsed') or entry.published_parsed is None:
                continue
            
            try:
                pub_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                current_date = datetime.now(timezone.utc)
                
                diff_seconds = (current_date - pub_date).total_seconds()
                diff_hours = diff_seconds / 3600
                if diff_hours < 0: diff_hours = 0
                
                pub_date_kst = pub_date + timedelta(hours=9)
                time_str_kst = pub_date_kst.strftime("%Y-%m-%d %H:%M:%S")

                source_name = "Google"
                if hasattr(entry, 'source') and hasattr(entry.source, 'title'):
                    source_name = entry.source.title

                if diff_hours > MAX_HOURS:
                    continue
                
                is_old_title = False
                for year in PAST_YEARS:
                    if year in entry.title:
                         is_old_title = True
                         print(f"ğŸ“… [êµ¬ê¸€|ì—°ë„íƒˆë½] {entry.title} (ì´ìœ : ê³¼ê±° ì—°ë„ '{year}' í¬í•¨)")
                         break
                if is_old_title: continue

                news_list.append({
                    "title": entry.title,
                    "link": entry.link,
                    "source": source_name,
                    "origin": "êµ¬ê¸€",
                    "time_str": time_str_kst,
                    "keyword": keyword
                })
                
            except:
                continue
                
    except Exception as e:
        print(f"âŒ êµ¬ê¸€ ì˜¤ë¥˜({keyword}): {e}")
        pass
        
    return news_list

# ---------------------------------------------------
# [í†µí•© í•¨ìˆ˜] ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì„ ë³„ (ê³¼ê±° ê¸°ë¡ ë¹„êµ ì¶”ê°€)
# ---------------------------------------------------
def collect_news(past_titles):
    print(f"\nğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì •ë°€ ì‹¬ì‚¬ ì‹œì‘ (ì œí•œ: {MAX_HOURS}ì‹œê°„)")
    all_news = []
    seen_links = set()
    collected_titles = [] 
    
    MAX_TOTAL = 20        
    MAX_PER_KEYWORD = 4
    DUPLICATE_THRESHOLD = 6
    
    for keyword in KEYWORDS:
        if len(all_news) >= MAX_TOTAL: 
            print("ğŸ›‘ [ì „ì²´ì œí•œ] ì´ 20ê°œë¥¼ ëª¨ë‘ ì±„ì›Œ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
            
        n_res = get_naver_news(keyword)
        g_res = get_google_news(keyword)
        
        current_keyword_count = 0
        
        for news in n_res + g_res:
            if len(all_news) >= MAX_TOTAL: break
            if current_keyword_count >= MAX_PER_KEYWORD: break
            
            # [1] ì°¨ë‹¨ ì‚¬ì´íŠ¸ í•„í„°
            is_excluded = False
            check_target = (news['link'] + news['title'] + news.get('source', '')).lower()
            
            for ban_word in EXCLUDE_LIST:
                if ban_word.lower() in check_target:
                    is_excluded = True
                    print(f"ğŸš« [ì‚¬ì´íŠ¸ì°¨ë‹¨][{news['origin']}][í‚¤ì›Œë“œ:{news['keyword']}] {news['title']} (ì´ìœ : {ban_word})") 
                    break
            
            if is_excluded: continue 

            # [2] ë§í¬ ì¤‘ë³µ í•„í„°
            if news['link'] in seen_links: continue

            clean_title = html.unescape(news['title']).replace("[", "").replace("]", "").strip()
            
            # [3] ì œëª© ë‚´ìš© ì¤‘ë³µ í•„í„° (ì˜¤ëŠ˜ ìˆ˜ì§‘í•œ ê²ƒë“¤ë¼ë¦¬ ë¹„êµ)
            is_similar = False
            for existing_title in collected_titles:
                if len(clean_title) < DUPLICATE_THRESHOLD: break
                for i in range(len(clean_title) - DUPLICATE_THRESHOLD + 1):
                    sub_string = clean_title[i : i + DUPLICATE_THRESHOLD]
                    if sub_string in existing_title:
                        is_similar = True
                        break 
                if is_similar: break
            
            if is_similar:
                print(f"ğŸ”— [ë‚´ìš©ì¤‘ë³µ][{news['origin']}][í‚¤ì›Œë“œ:{news['keyword']}] {clean_title}")
                continue
            
            # [4] â˜… ê³¼ê±° ê¸°ë¡(ì–´ì œ ë‰´ìŠ¤) ì¤‘ë³µ í•„í„° (ì¶”ê°€ë¨) â˜…
            is_past_duplicate = False
            for past_title in past_titles:
                # ê³¼ê±° ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ íŒ¨ìŠ¤
                if len(clean_title) < DUPLICATE_THRESHOLD or len(past_title) < DUPLICATE_THRESHOLD:
                    break
                
                # 10ê¸€ì ì´ìƒ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
                for i in range(len(clean_title) - DUPLICATE_THRESHOLD + 1):
                    sub_string = clean_title[i : i + DUPLICATE_THRESHOLD]
                    if sub_string in past_title:
                        is_past_duplicate = True
                        break
                if is_past_duplicate: break
                
            if is_past_duplicate:
                print(f"ğŸ§Ÿ [ì–´ì œë‰´ìŠ¤ì¤‘ë³µ] {clean_title} (ì–´ì œ ì´ë¯¸ ì „ì†¡ë¨)")
                continue

            # [5] ìµœì¢… í•©ê²©
            print(f"âœ… [ìµœì¢…ì„ ë³„][{news['origin']}][í‚¤ì›Œë“œ:{news['keyword']}] {clean_title} (ì‘ì„±ì‹œê°„: {news.get('time_str', 'ì•Œìˆ˜ì—†ìŒ')})")
            
            all_news.append({"title": clean_title, "link": news['link']})
            seen_links.add(news['link'])
            collected_titles.append(clean_title)
            current_keyword_count += 1
                
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(all_news)}ê°œ ë‰´ìŠ¤ ì „ì†¡ ì¤€ë¹„ ì™„ë£Œ\n")
    return all_news

# ---------------------------------------------------
# [ì „ì†¡ ë¡œì§]
# ---------------------------------------------------
async def send_newsletter(target_channel_id, news_data):
    channel = bot.get_channel(target_channel_id)
    if not channel:
        print(f"âŒ ì±„ë„ ì—†ìŒ: {target_channel_id}")
        return

    if not news_data:
        return

    today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    MAX_DESCRIPTION_LEN = 3500
    current_description = ""
    page_count = 1
    
    embed = discord.Embed(title=f"ğŸ® {today} ì´ìŠ¤í¬ì¸  ì£¼ìš” ì†Œì‹", color=0x00ff00)

    for idx, news in enumerate(news_data):
        one_line = f"` {idx+1}. ` [{news['title']}]({news['link']})\n\n"
        
        if len(current_description) + len(one_line) > MAX_DESCRIPTION_LEN:
            embed.description = current_description
            embed.set_footer(text=f"HantaGG NewsBot â€¢ {page_count}í˜ì´ì§€")
            await channel.send(embed=embed)
            page_count += 1
            current_description = ""
            embed = discord.Embed(color=0x00ff00)
            
        current_description += one_line

    if current_description:
        embed.description = current_description
        embed.set_footer(text=f"HantaGG NewsBot â€¢ ë§ˆì§€ë§‰ í˜ì´ì§€ (ì´ {len(news_data)}ê±´)")
        await channel.send(embed=embed)

    print(f"âœ… ì „ì†¡ ì™„ë£Œ: {target_channel_id}")

# ---------------------------------------------------
# [ë´‡ ì‹¤í–‰]
# ---------------------------------------------------
@bot.event
async def on_ready():
    print(f"âœ… ë´‡ ë¡œê·¸ì¸: {bot.user}")
    
    try:
        # 1. ë´‡ì´ ê¸°ì–µì„ ë˜ì‚´ë¦½ë‹ˆë‹¤ (ì–´ì œ ë³´ë‚¸ ë‰´ìŠ¤ ì œëª© ê°€ì ¸ì˜¤ê¸°)
        # TARGET_CHANNELSì˜ ì²« ë²ˆì§¸ ì±„ë„ì„ ê¸°ì¤€ìœ¼ë¡œ ê¸°ë¡ì„ í™•ì¸í•©ë‹ˆë‹¤.
        past_titles = []
        if TARGET_CHANNELS:
            past_titles = await get_past_titles(TARGET_CHANNELS[0])
            
        # 2. ì–´ì œ ê¸°ë¡(past_titles)ì„ ì „ë‹¬í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        todays_news = collect_news(past_titles)
        
        # 3. ì „ì†¡
        for channel_id in TARGET_CHANNELS:
            await send_newsletter(channel_id, todays_news)
            
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    print("ğŸ‘‹ ì„ë¬´ ì™„ë£Œ. ì¢…ë£Œí•©ë‹ˆë‹¤.")
    await bot.close()

if __name__ == "__main__":
    bot.run(DISCORD_TOKEN)




