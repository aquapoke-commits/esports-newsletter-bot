import discord
from discord.ext import commands
import requests
from bs4 import BeautifulSoup
import feedparser
import html
import os
from datetime import datetime, timedelta, timezone
import asyncio

# =====================================================================
# [ë³´ì•ˆ ì„¤ì •]
# =====================================================================
if 'DISCORD_TOKEN' in os.environ:
    DISCORD_TOKEN = os.environ['DISCORD_TOKEN']
else:
    print("âš ï¸ ì—ëŸ¬: DISCORD_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# [ì„¤ì •] ì±„ë„ ID ë¦¬ìŠ¤íŠ¸
TARGET_CHANNELS = [
    1447898781365567580, # GGX Proto
    1450833963278012558, # Hanta.GG
    # 987654321098765432,  # í…ŒìŠ¤íŠ¸ìš©
]

# [ì„¤ì •] ê²€ìƒ‰ì–´ ëª©ë¡
KEYWORDS = ["ì´ìŠ¤í¬ì¸ ", "LCK", "VCT", "ì´í„°ë„ ë¦¬í„´ ì´ìŠ¤í¬ì¸ ", "PUBG", "í‹°ì›", "Faker", "Gen.G", "HLE", "kt Rolster", "ë””í”ŒëŸ¬ìŠ¤ ê¸°ì•„", "í”¼ì–´ì—‘ìŠ¤", "ë†ì‹¬ ë ˆë“œí¬ìŠ¤", "í•œì§„ ë¸Œë¦¬ì˜¨", "DRX", "DN SOOPers"]

# [ì„¤ì •] ì°¨ë‹¨í•  ë‹¨ì–´ (ì†Œë¬¸ì)
EXCLUDE_LIST = ["theqoo", "ë”ì¿ ", "instiz", "ì¸ìŠ¤í‹°ì¦ˆ", "fmkorea", "í¨ì½”", "dcinside", "ë””ì‹œ", "ë°”ì¹´ë¼", "í† í† ", "ì¹´ì§€ë…¸", "ìŠ¬ë¡¯"]

# [ì„¤ì •] ë‰´ìŠ¤ ìœ íš¨ ì‹œê°„ (ë‹¨ìœ„: ì‹œê°„)
MAX_HOURS = 24

# ë´‡ ì„¤ì •
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix="!", intents=intents)

# ---------------------------------------------------
# [í¬ë¡¤ë§ í•¨ìˆ˜ 1] ë„¤ì´ë²„ ë‰´ìŠ¤
# ---------------------------------------------------
def get_naver_news(keyword):
    news_list = []
    clean_keyword = keyword.replace(" ", "+")
    url = f"https://search.naver.com/search.naver?where=news&query={clean_keyword}&sort=1"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ì•ˆì „í•œ ë¦¬ìŠ¤íŠ¸ ê²€ìƒ‰
        items = soup.select('ul.list_news > li.bx')
        
        for item in items:
            title_tag = item.select_one('a.news_tit')
            if not title_tag: continue
            
            title = title_tag.text
            link = title_tag['href']
            
            # [Naver ì‹œê°„ ì •ë°€ ê²€ì‚¬]
            info_group = item.select('.info_group .info')
            is_recent = False
            time_log = "ì•Œìˆ˜ì—†ìŒ"
            
            for info in info_group:
                text = info.text
                if "ë¶„ ì „" in text or "ì‹œê°„ ì „" in text:
                    time_log = text 
                    if "ì¼ ì „" in text:
                        is_recent = False
                        break
                    is_recent = True
                    break
            
            if is_recent:
                news_list.append({
                    "title": title, 
                    "link": link, 
                    "source": "Naver", 
                    "origin": "ë„¤ì´ë²„",
                    "time_str": time_log,
                    "keyword": keyword # [ì¶”ê°€] ì–´ë–¤ í‚¤ì›Œë“œë¡œ ì°¾ì•˜ëŠ”ì§€ ì €ì¥
                })

    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ ì˜¤ë¥˜({keyword}): {e}")
        pass
    return news_list

# ---------------------------------------------------
# [í¬ë¡¤ë§ í•¨ìˆ˜ 2] êµ¬ê¸€ ë‰´ìŠ¤
# ---------------------------------------------------
def get_google_news(keyword):
    news_list = []
    clean_keyword = keyword.replace(" ", "+")
    url = f"https://news.google.com/rss/search?q={clean_keyword}+when:1d&hl=ko&gl=KR&ceid=KR:ko"
    
    # [ì—°ë„ í•„í„°]
    PAST_YEARS = ["2020", "2021", "2022", "2023", "2024", "2025"] 

    try:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            if not hasattr(entry, 'published_parsed') or entry.published_parsed is None:
                continue
            
            try:
                # 1. ì‹œê°„ ê³„ì‚° (UTC)
                pub_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                current_date = datetime.now(timezone.utc)
                
                diff_seconds = (current_date - pub_date).total_seconds()
                diff_hours = diff_seconds / 3600
                if diff_hours < 0: diff_hours = 0
                
                pub_date_kst = pub_date + timedelta(hours=9)
                time_str_kst = pub_date_kst.strftime("%Y-%m-%d %H:%M:%S")

                source_name = "Google"
                if hasattr(entry, 'source') and hasattr(entry.source, 'title'):
                    source_name = entry.source.title

                # [ì‹œê°„ ì œí•œ]
                if diff_hours > MAX_HOURS:
                    # print(f"â° [êµ¬ê¸€|íƒˆë½] {keyword} | {entry.title} (ì‘ì„±ì‹œê°„: {time_str_kst})")
                    continue
                
                # [ì—°ë„ í•„í„°]
                is_old_title = False
                for year in PAST_YEARS:
                    if year in entry.title:
                         is_old_title = True
                         print(f"ğŸ“… [êµ¬ê¸€|ì—°ë„íƒˆë½] {entry.title} (ì´ìœ : ê³¼ê±° ì—°ë„ '{year}' í¬í•¨)")
                         break
                if is_old_title: continue

                news_list.append({
                    "title": entry.title,
                    "link": entry.link,
                    "source": source_name,
                    "origin": "êµ¬ê¸€",
                    "time_str": time_str_kst,
                    "keyword": keyword # [ì¶”ê°€] ì–´ë–¤ í‚¤ì›Œë“œë¡œ ì°¾ì•˜ëŠ”ì§€ ì €ì¥
                })
                
            except:
                continue
                
    except Exception as e:
        print(f"âŒ êµ¬ê¸€ ì˜¤ë¥˜({keyword}): {e}")
        pass
        
    return news_list

# ---------------------------------------------------
# [í†µí•© í•¨ìˆ˜] ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì„ ë³„
# ---------------------------------------------------
def collect_news():
    print(f"\nğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì •ë°€ ì‹¬ì‚¬ ì‹œì‘ (ì œí•œ: {MAX_HOURS}ì‹œê°„)")
    all_news = []
    seen_links = set()
    collected_titles = [] 
    
    MAX_TOTAL = 20        
    MAX_PER_KEYWORD = 4
    DUPLICATE_THRESHOLD = 10
    
    for keyword in KEYWORDS:
        if len(all_news) >= MAX_TOTAL: 
            print("ğŸ›‘ [ì „ì²´ì œí•œ] ì´ 20ê°œë¥¼ ëª¨ë‘ ì±„ì›Œ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
            
        n_res = get_naver_news(keyword)
        g_res = get_google_news(keyword)
        
        current_keyword_count = 0
        
        for news in n_res + g_res:
            if len(all_news) >= MAX_TOTAL: break
            
            if current_keyword_count >= MAX_PER_KEYWORD: 
                break
            
            # [1] ì°¨ë‹¨ ì‚¬ì´íŠ¸ í•„í„°
            is_excluded = False
            check_target = (news['link'] + news['title'] + news.get('source', '')).lower()
            
            for ban_word in EXCLUDE_LIST:
                if ban_word.lower() in check_target:
                    is_excluded = True
                    print(f"ğŸš« [ì‚¬ì´íŠ¸ì°¨ë‹¨][{news['origin']}][í‚¤ì›Œë“œ:{news['keyword']}] {news['title']} (ì´ìœ : {ban_word})") 
                    break
            
            if is_excluded: continue 

            # [2] ë§í¬ ì¤‘ë³µ í•„í„°
            if news['link'] in seen_links: 
                continue

            clean_title = html.unescape(news['title']).replace("[", "").replace("]", "").strip()
            
            # [3] ì œëª© ë‚´ìš© ì¤‘ë³µ í•„í„°
            is_similar = False
            for existing_title in collected_titles:
                if len(clean_title) < DUPLICATE_THRESHOLD: break
                for i in range(len(clean_title) - DUPLICATE_THRESHOLD + 1):
                    sub_string = clean_title[i : i + DUPLICATE_THRESHOLD]
                    if sub_string in existing_title:
                        is_similar = True
                        break 
                if is_similar: break

            if is_similar:
                print(f"ğŸ”— [ë‚´ìš©ì¤‘ë³µ][{news['origin']}][í‚¤ì›Œë“œ:{news['keyword']}] {clean_title}")
                continue

            # [4] ìµœì¢… í•©ê²© - í‚¤ì›Œë“œ ì •ë³´ ì¶œë ¥ ì¶”ê°€
            print(f"âœ… [ìµœì¢…ì„ ë³„][{news['origin']}][í‚¤ì›Œë“œ:{news['keyword']}] {clean_title} (ì‘ì„±ì‹œê°„: {news.get('time_str', 'ì•Œìˆ˜ì—†ìŒ')})")
            
            all_news.append({"title": clean_title, "link": news['link']})
            seen_links.add(news['link'])
            collected_titles.append(clean_title)
            current_keyword_count += 1
                
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(all_news)}ê°œ ë‰´ìŠ¤ ì „ì†¡ ì¤€ë¹„ ì™„ë£Œ\n")
    return all_news

# ---------------------------------------------------
# [ì „ì†¡ ë¡œì§]
# ---------------------------------------------------
async def send_newsletter(target_channel_id, news_data):
    channel = bot.get_channel(target_channel_id)
    if not channel:
        print(f"âŒ ì±„ë„ ì—†ìŒ: {target_channel_id}")
        return

    if not news_data:
        return

    today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    MAX_DESCRIPTION_LEN = 3500
    current_description = ""
    page_count = 1
    
    embed = discord.Embed(title=f"ğŸ® {today} ì´ìŠ¤í¬ì¸  ì£¼ìš” ì†Œì‹", color=0x00ff00)

    for idx, news in enumerate(news_data):
        one_line = f"` {idx+1}. ` [{news['title']}]({news['link']})\n\n"
        
        if len(current_description) + len(one_line) > MAX_DESCRIPTION_LEN:
            embed.description = current_description
            embed.set_footer(text=f"HantaGG NewsBot â€¢ {page_count}í˜ì´ì§€")
            await channel.send(embed=embed)
            page_count += 1
            current_description = ""
            embed = discord.Embed(color=0x00ff00)
            
        current_description += one_line

    if current_description:
        embed.description = current_description
        embed.set_footer(text=f"HantaGG NewsBot â€¢ ë§ˆì§€ë§‰ í˜ì´ì§€ (ì´ {len(news_data)}ê±´)")
        await channel.send(embed=embed)

    print(f"âœ… ì „ì†¡ ì™„ë£Œ: {target_channel_id}")

# ---------------------------------------------------
# [ë´‡ ì‹¤í–‰]
# ---------------------------------------------------
@bot.event
async def on_ready():
    print(f"âœ… ë´‡ ë¡œê·¸ì¸: {bot.user}")
    
    try:
        todays_news = collect_news()
        for channel_id in TARGET_CHANNELS:
            await send_newsletter(channel_id, todays_news)
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    print("ğŸ‘‹ ì„ë¬´ ì™„ë£Œ. ì¢…ë£Œí•©ë‹ˆë‹¤.")
    await bot.close()

if __name__ == "__main__":
    bot.run(DISCORD_TOKEN)
