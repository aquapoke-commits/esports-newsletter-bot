import discord
from discord.ext import commands
import requests
from bs4 import BeautifulSoup
import feedparser
import html
import os
# [ì¤‘ìš”] ì •í™•í•œ ì‹œê°„ ê³„ì‚°ì„ ìœ„í•´ timezone í•„ìˆ˜
from datetime import datetime, timedelta, timezone
import asyncio

# =====================================================================
# [ë³´ì•ˆ ì„¤ì •]
# =====================================================================
if 'DISCORD_TOKEN' in os.environ:
    DISCORD_TOKEN = os.environ['DISCORD_TOKEN']
else:
    print("âš ï¸ ì—ëŸ¬: DISCORD_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# [ì„¤ì •] ì±„ë„ ID ë¦¬ìŠ¤íŠ¸
TARGET_CHANNELS = [
    1447898781365567580, # GGX Proto
    1450833963278012558, # Hanta.GG
    987654321098765432,  # í…ŒìŠ¤íŠ¸ìš©
]

# [ì„¤ì •] ê²€ìƒ‰ì–´ ëª©ë¡
KEYWORDS = ["ì´ìŠ¤í¬ì¸ ", "LCK", "VCT", "ì´í„°ë„ ë¦¬í„´ ì´ìŠ¤í¬ì¸ ", "PUBG", "í‹°ì›", "Faker", "Gen.G", "HLE", "kt Rolster", "ë””í”ŒëŸ¬ìŠ¤ ê¸°ì•„", "í”¼ì–´ì—‘ìŠ¤", "ë†ì‹¬ ë ˆë“œí¬ìŠ¤", "í•œì§„ ë¸Œë¦¬ì˜¨", "DRX", "DN SOOPers"]

# [ì„¤ì •] ì°¨ë‹¨í•  ë‹¨ì–´ (ì†Œë¬¸ì)
EXCLUDE_LIST = ["theqoo", "ë”ì¿ ", "instiz", "fmkorea", "dcinside"]

# [ì„¤ì •] ë‰´ìŠ¤ ìœ íš¨ ì‹œê°„ (ë‹¨ìœ„: ì‹œê°„)
# 24ì‹œê°„ì´ ë„ˆë¬´ ë„ë„í•˜ë©´ 18~20ì‹œê°„ìœ¼ë¡œ ì¤„ì´ì„¸ìš”.
MAX_HOURS = 24

# ë´‡ ì„¤ì •
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix="!", intents=intents)

# ---------------------------------------------------
# [í¬ë¡¤ë§ í•¨ìˆ˜] - ì‘ì„± ì‹œê°„ ë¡œê·¸ ì¶”ê°€
# ---------------------------------------------------
def get_naver_news(keyword):
    news_list = []
    clean_keyword = keyword.replace(" ", "+")
    url = f"https://search.naver.com/search.naver?where=news&query={clean_keyword}&sort=1"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')
        items = soup.select('.news_wrap')
        
        for item in items:
            title = item.select_one('.news_tit').text
            link = item.select_one('.news_tit')['href']
            
            # [Naver ì‹œê°„ ì •ë°€ ê²€ì‚¬]
            info_group = item.select('.info_group .info')
            is_recent = False
            time_log = "ì•Œìˆ˜ì—†ìŒ" # ë¡œê·¸ìš© ë³€ìˆ˜
            
            for info in info_group:
                text = info.text
                if "ë¶„ ì „" in text or "ì‹œê°„ ì „" in text:
                    time_log = text # ì˜ˆ: "1ì‹œê°„ ì „" ì €ì¥
                    if "ì¼ ì „" in text:
                        print(f"â° [ë„¤ì´ë²„|íƒˆë½] {keyword} | {title} (ì‘ì„±ì‹œê°„: {text} - ìˆ˜ì •ëœ êµ¬ ê¸°ì‚¬)")
                        is_recent = False
                        break
                    is_recent = True
                    break
            
            if is_recent:
                news_list.append({
                    "title": title, 
                    "link": link, 
                    "source": "Naver", 
                    "origin": "ë„¤ì´ë²„",
                    "time_str": time_log # ì‘ì„± ì‹œê°„ ì •ë³´ ì €ì¥
                })

    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ ì˜¤ë¥˜({keyword}): {e}")
        pass
    return news_list

def get_google_news(keyword):
    news_list = []
    clean_keyword = keyword.replace(" ", "+")
    url = f"https://news.google.com/rss/search?q={clean_keyword}+when:1d&hl=ko&gl=KR&ceid=KR:ko"
    
    try:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            if not hasattr(entry, 'published_parsed') or entry.published_parsed is None:
                continue
            
            try:
                # 1. ì‹œê°„ ê³„ì‚° (UTC ê¸°ì¤€)
                pub_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                current_date = datetime.now(timezone.utc)
                
                diff_seconds = (current_date - pub_date).total_seconds()
                diff_hours = diff_seconds / 3600
                if diff_hours < 0: diff_hours = 0
                
                # 2. ë¡œê·¸ ì¶œë ¥ì„ ìœ„í•´ í•œêµ­ ì‹œê°„(KST)ìœ¼ë¡œ ë³€í™˜
                pub_date_kst = pub_date + timedelta(hours=9)
                time_str_kst = pub_date_kst.strftime("%Y-%m-%d %H:%M:%S")

                source_name = "Google"
                if hasattr(entry, 'source') and hasattr(entry.source, 'title'):
                    source_name = entry.source.title

                if diff_hours > MAX_HOURS:
                    print(f"â° [êµ¬ê¸€|íƒˆë½] {keyword} | {entry.title} (ì‘ì„±ì‹œê°„: {time_str_kst} | {diff_hours:.1f}ì‹œê°„ ì „)")
                    continue
                
                news_list.append({
                    "title": entry.title,
                    "link": entry.link,
                    "source": source_name,
                    "origin": "êµ¬ê¸€",
                    "time_str": time_str_kst # í•œêµ­ ì‹œê°„ ë¬¸ìì—´ ì €ì¥
                })
                
            except:
                continue
                
    except Exception as e:
        print(f"âŒ êµ¬ê¸€ ì˜¤ë¥˜({keyword}): {e}")
        pass
        
    return news_list

def collect_news():
    print(f"\nğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì •ë°€ ì‹¬ì‚¬ ì‹œì‘ (ì œí•œ: {MAX_HOURS}ì‹œê°„)")
    all_news = []
    seen_links = set()
    collected_titles = [] 
    
    MAX_TOTAL = 20        
    MAX_PER_KEYWORD = 4
    DUPLICATE_THRESHOLD = 10
    
    for keyword in KEYWORDS:
        if len(all_news) >= MAX_TOTAL: 
            print("ğŸ›‘ [ì „ì²´ì œí•œ] ì´ 20ê°œë¥¼ ëª¨ë‘ ì±„ì›Œ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
            
        n_res = get_naver_news(keyword)
        g_res = get_google_news(keyword)
        
        current_keyword_count = 0
        
        for news in n_res + g_res:
            if len(all_news) >= MAX_TOTAL: break
            
            if current_keyword_count >= MAX_PER_KEYWORD: 
                break
            
            # [1] ì°¨ë‹¨ ì‚¬ì´íŠ¸ í•„í„°
            is_excluded = False
            check_target = (news['link'] + news['title'] + news.get('source', '')).lower()
            
            for ban_word in EXCLUDE_LIST:
                if ban_word.lower() in check_target:
                    is_excluded = True
                    print(f"ğŸš« [ì‚¬ì´íŠ¸ì°¨ë‹¨][{news['origin']}] {news['title']} (ì´ìœ : {ban_word})") 
                    break
            
            if is_excluded: continue 

            # [2] ë§í¬ ì¤‘ë³µ í•„í„°
            if news['link'] in seen_links: 
                continue

            clean_title = html.unescape(news['title']).replace("[", "").replace("]", "").strip()
            
            # [3] ì œëª© ë‚´ìš© ì¤‘ë³µ í•„í„°
            is_similar = False
            for existing_title in collected_titles:
                if len(clean_title) < DUPLICATE_THRESHOLD: break
                for i in range(len(clean_title) - DUPLICATE_THRESHOLD + 1):
                    sub_string = clean_title[i : i + DUPLICATE_THRESHOLD]
                    if sub_string in existing_title:
                        is_similar = True
                        break 
                if is_similar: break

            if is_similar:
                print(f"ğŸ”— [ë‚´ìš©ì¤‘ë³µ][{news['origin']}] {clean_title}")
                continue

            # [4] ìµœì¢… í•©ê²© - ì‘ì„± ì‹œê°„ ì •ë³´(time_str) í•¨ê»˜ ì¶œë ¥
            print(f"âœ… [ìµœì¢…ì„ ë³„][{news['origin']}] {clean_title} (ì‘ì„±ì‹œê°„: {news.get('time_str', 'ì•Œìˆ˜ì—†ìŒ')})")
            
            all_news.append({"title": clean_title, "link": news['link']})
            seen_links.add(news['link'])
            collected_titles.append(clean_title)
            current_keyword_count += 1
                
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(all_news)}ê°œ ë‰´ìŠ¤ ì „ì†¡ ì¤€ë¹„ ì™„ë£Œ\n")
    return all_news
    
# ---------------------------------------------------
# [ì „ì†¡ ë¡œì§]
# ---------------------------------------------------
async def send_newsletter(target_channel_id, news_data):
    channel = bot.get_channel(target_channel_id)
    if not channel:
        print(f"âŒ ì±„ë„ ì—†ìŒ: {target_channel_id}")
        return

    if not news_data:
        return

    today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    MAX_DESCRIPTION_LEN = 3500
    current_description = ""
    page_count = 1
    
    embed = discord.Embed(title=f"ğŸ® {today} ì´ìŠ¤í¬ì¸  ì£¼ìš” ì†Œì‹", color=0x00ff00)

    for idx, news in enumerate(news_data):
        one_line = f"` {idx+1}. ` [{news['title']}]({news['link']})\n\n"
        
        if len(current_description) + len(one_line) > MAX_DESCRIPTION_LEN:
            embed.description = current_description
            embed.set_footer(text=f"HantaGG NewsBot â€¢ {page_count}í˜ì´ì§€")
            await channel.send(embed=embed)
            page_count += 1
            current_description = ""
            embed = discord.Embed(color=0x00ff00)
            
        current_description += one_line

    if current_description:
        embed.description = current_description
        embed.set_footer(text=f"HantaGG NewsBot â€¢ ë§ˆì§€ë§‰ í˜ì´ì§€ (ì´ {len(news_data)}ê±´)")
        await channel.send(embed=embed)

    print(f"âœ… ì „ì†¡ ì™„ë£Œ: {target_channel_id}")

# ---------------------------------------------------
# [ë´‡ ì‹¤í–‰]
# ---------------------------------------------------
@bot.event
async def on_ready():
    print(f"âœ… ë´‡ ë¡œê·¸ì¸: {bot.user}")
    
    todays_news = collect_news()
    
    for channel_id in TARGET_CHANNELS:
        await send_newsletter(channel_id, todays_news)
    
    print("ğŸ‘‹ ì„ë¬´ ì™„ë£Œ. ì¢…ë£Œí•©ë‹ˆë‹¤.")
    await bot.close()

if __name__ == "__main__":
    bot.run(DISCORD_TOKEN)





